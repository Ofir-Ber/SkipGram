{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d30fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "# useful stuff\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "from nltk import stem\n",
    "from string import digits\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import json\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aeee75",
   "metadata": {},
   "source": [
    "Preprocessing and load text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6c4bb",
   "metadata": {},
   "source": [
    "Text Cleaning - First, as python is a case sensitive language, I put every word in lowercase, then I did some classic processing such as removing:\n",
    "-\tPunctuation\n",
    "-\tDigits\n",
    "-\tHyphens\n",
    "-\t‘s and ‘t\n",
    "-\t‘empty’ words (words composed of only one letter) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c424db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sentences(path):\n",
    "    # feel free to make a better tokenization/pre-processing\n",
    "    sentences = []\n",
    "    stemmed_word_list = []\n",
    "    stemmer=stem.snowball.EnglishStemmer()\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for l in f:\n",
    "            sent = l.lower().split()\n",
    "            sent_new = [word.translate(str.maketrans('','','''!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~''')) for word in sent]\n",
    "            sent_new_1 = [word.translate(str.maketrans('', '', digits)) for word in sent_new] # clean digits\n",
    "            sent_new_2 = [word.replace('-','') for word in sent_new_1] # clean hyphen \n",
    "            sent_new_3 = [word.replace(\"'s\",'') for word in sent_new_2] # clean 's\n",
    "            sent_new_4 = [word.replace(\"'t\",'') for word in sent_new_3] # clean 't\n",
    "            sent_new_5 = [word for word in sent_new_4 if len(word)>1] # clean empty 'words'\n",
    "            sentences.append(sent_new_5)\n",
    "    # Stemming\n",
    "    #for s in sentences:\n",
    "    #    stemmed_sentences = []\n",
    "    #    for word in s:\n",
    "    #        stemmed_sentences.append(stemmer.stem(word))\n",
    "    #    stemmed_word_list.append(stemmed_sentences)\n",
    "    \n",
    "    #return stemmed_word_list\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879564c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPairs(path):\n",
    "    data = pd.read_csv(path, delimiter='\\t')\n",
    "    pairs = zip(data['word1'],data['word2'],data['similarity'])\n",
    "    return pairs\n",
    "\n",
    "def drop(u, v):\n",
    "    return u - v * u.dot(v)/v.dot(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1446de",
   "metadata": {},
   "source": [
    "**SkipGram model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbef33",
   "metadata": {},
   "source": [
    "Functions - \n",
    "1)\tUnigram distribution - \n",
    "The idea behind unigram distribution was that the probability of picking a word should be equal to the number of times this word appears in the corpus raised to the power ¾ , divided by the total number of words in the corpus also raised to the power ¾ as it is done in the original paper.\n",
    "\n",
    "My function generates a large table that contains all the words repeated as many times as their distribution. I will then randomly sample the negative words from this unigram table.\n",
    "\n",
    "2)\tWord frequency -\n",
    "Filtering stopwords, words below a minimum count and words that are not in vocab.\n",
    "\n",
    "3)\tWord to ID mapping -\n",
    "Preparing Word to ID input mapping vector\n",
    "\n",
    "4)\tPositive words -\n",
    "Words that actually appear within the context window of the center word. A word vector of a center word will be more similar to a positive word than of randomly drawn negative words because words appearing together have strong correlation. \n",
    "\n",
    "5)\tNegative sampling\n",
    "The negative sampling is performed by randomly picking word from the unigram table taking into account that the sampled word is neither part of the context nor the center word with the length of the negative word set predefined as a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232eae7",
   "metadata": {},
   "source": [
    "Training - \n",
    "Forward Propagation: Computing hidden (projection) layer\n",
    "Forward Propagation: Sigmoid output layer\n",
    "Backward Propagation: Prediction Error\n",
    "Backward Propagation: Computing ∇ Winput\n",
    "Backward Propagation: Computing ∇ Woutput\n",
    "Backward Propagation: Updating Weight Matrices\n",
    "\n",
    "Similarity – \n",
    "For the computation of the similarity, I chose the cosine similarity which is defined as the cosine of the angle between two sequences of numbers or the dot product of the vectors divided by the product of their lengths.\n",
    "Given the fact that we are in a positive space, it returns a number between [0 ; 1] indicating the similarity (the higher the more similar the words are). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85e5f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, sentences, nEmbed=100, negativeRate=5, winSize = 5, minCount = 5, epochs=1, lr=0.1):\n",
    "        self.w2id = {} # word to ID mapping\n",
    "        self.trainset = sentences # set of sentences\n",
    "        self.minCount = minCount # Minimum occurrence of words\n",
    "        words = [word for sentence in sentences for word in sentence] # Store every word appearing in the trainset with repetitions\n",
    "        word_count = dict(Counter(words)) # Return the frequency of words in the set\n",
    "\n",
    "        # filter words with minCount\n",
    "        word_count = {word:freq for word, freq in word_count.items() if freq >= self.minCount}\n",
    "        words = [word for word, freq in word_count.items()]\n",
    "\n",
    "        self.vocab = sorted(set(words)) # list of valid words\n",
    "        vocab_size = len(self.vocab) # Size of the vocabulary\n",
    "        word_count = dict(Counter(words)) # Return the frequency of words in the set\n",
    "        \n",
    "        self.freq = np.array([word_count[word] / vocab_size for word in self.vocab]) # frequency of words, to be used for the unigram\n",
    "        self.nEmbed = nEmbed\n",
    "        self.negativeRate = negativeRate\n",
    "        self.winSize = winSize\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr # Learning rate\n",
    "        self.accLoss = 0\n",
    "        self.trainWords = 0\n",
    "        self.loss = [] # list that will store the loss values\n",
    "        self.loss_epoch = [] # list that will store the loss values of a particular epoch\n",
    "\n",
    "        # Initialization -- > Try others\n",
    "        #self.centerV = (np.random.rand(vocab_size, nEmbed) - 0.5) / nEmbed # Initialize random vector for center words\n",
    "        #self.contxtV = (np.random.rand(vocab_size, nEmbed) - 0.5) / nEmbed # Initialize random vector for context words\n",
    "\n",
    "        # He Initialisation - np.random.randn(a, b) * np.sqrt(2/b)\n",
    "        #self.centerV = np.random.rand(vocab_size, nEmbed) * np.sqrt(2/nEmbed) # Initialize random vector for center words\n",
    "        #self.contxtV = np.random.rand(vocab_size, nEmbed) * np.sqrt(2/nEmbed) # Initialize random vector for context words\n",
    "\n",
    "        self.centerV = np.random.randn(vocab_size, nEmbed)\n",
    "        self.contxtV = np.random.randn(vocab_size, nEmbed)\n",
    "        \n",
    "        #self.centerV = np.random.randn(vocab_size, nEmbed)\n",
    "        #self.contxtV = np.random.randn(vocab_size, nEmbed)\n",
    "        \n",
    "        ## Prepare w2id: word to index\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.w2id[word] = i\n",
    "\n",
    "        # Create the unigram table\n",
    "        self.unigram_table = self.compute_unigram_table()\n",
    "\n",
    "        #print(\"Initalization completed!\")\n",
    "\n",
    "    def compute_unigram_table(self, exponent=0.75, table_length = int(1e8)):\n",
    "        \"\"\" Compute the unigram table \"\"\"\n",
    "        # Length of the unigram table is equal to 1e8\n",
    "        # This large table will contain every words, repeated as many times as their frequency\n",
    "        # The negative sampling will be done from this table\n",
    "\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Normalization factor for the word probabilities = denominator\n",
    "        norm_factor = sum(\n",
    "            [np.power(self.freq[i], exponent) for i in range(vocab_size)]\n",
    "        )\n",
    "\n",
    "        table = np.array(np.zeros(table_length), dtype=int)\n",
    "        p = 0 # Cumulative probability\n",
    "        count = 0\n",
    "\n",
    "        # iterate over every words\n",
    "        for i in range(vocab_size):\n",
    "            p += np.power(self.freq[i], exponent) / norm_factor\n",
    "\n",
    "            while (count < table_length) and (count/ table_length < p):\n",
    "                table[count]= i\n",
    "                count +=1\n",
    "\n",
    "        np.random.shuffle(table)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "    def sample(self, omit):\n",
    "        #\"\"\"samples negative words, ommitting those in set omit\"\"\"\n",
    "        count = 0\n",
    "        negWordsId = []\n",
    "        # Sample negative words until we reach negativeRate\n",
    "        while count < self.negativeRate:\n",
    "            negWordId = np.random.choice(self.unigram_table)\n",
    "            if negWordId not in omit: #selecting only words that are not in omit (i.e. not in the current context)\n",
    "                negWordsId.append(negWordId)\n",
    "                count +=1\n",
    "        return negWordsId\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        print(\"Entering the training phase.\")\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            start_time = datetime.now()\n",
    "            self.loss_epoch = []\n",
    "            print(\"\\n epoch: %d of %d\" % (epoch + 1, self.epochs))\n",
    "            for counter, sent in enumerate(self.trainset):\n",
    "                sentence = [word for word  in sent if word in self.vocab]\n",
    "\n",
    "                for wpos, word in enumerate(sentence):\n",
    "                    wIdx = self.w2id[word]\n",
    "                    winsize = np.random.randint(self.winSize) + 1\n",
    "                    start = max(0, wpos - winsize)\n",
    "                    end = min(wpos + winsize + 1, len(sentence))\n",
    "\n",
    "                    for context_word in sentence[start:end]:\n",
    "                        ctxtId = self.w2id[context_word]\n",
    "                        if ctxtId == wIdx: continue\n",
    "                        negativeIds = self.sample({wIdx, ctxtId})\n",
    "                        self.trainWord(wIdx, ctxtId, negativeIds)\n",
    "                        self.trainWords += 1\n",
    "\n",
    "                if counter % 1000 == 0:\n",
    "                    end_time = datetime.now()\n",
    "                    print(' > training %d of %d' % (counter, len(self.trainset)), '- Duration: {}'.format(end_time - start_time))\n",
    "                \n",
    "                    self.loss.append(self.accLoss / self.trainWords)\n",
    "                    self.loss_epoch.append(self.accLoss / self.trainWords)\n",
    "                    self.trainWords = 0\n",
    "                    self.accLoss = 0.\n",
    "            print(\"Epoch:\", epoch + 1, \"Loss:\", sum(self.loss_epoch))\n",
    "\n",
    "    def trainWord(self, wordId, contextId, negativeIds):\n",
    "        vector = self.centerV[wordId]\n",
    "        ctxtvector = self.contxtV[contextId]\n",
    "        negvector = self.contxtV[negativeIds]\n",
    "        z = expit(-np.dot(ctxtvector, vector)) # logistic sigmoid (i.e inverse of the logit function)\n",
    "        zNeg = - expit(np.dot(negvector, vector))\n",
    "\n",
    "        ## Compute the gradients\n",
    "        contxtGrad = z * vector\n",
    "        centerGrad = z * self.contxtV[contextId] + np.dot(zNeg, negvector)\n",
    "        negGrad = np.outer(zNeg, vector)\n",
    "\n",
    "        ## Gradient descent step\n",
    "        np.add(vector, centerGrad * self.lr, out=vector);\n",
    "        np.add(ctxtvector, contxtGrad * self.lr, out=ctxtvector);\n",
    "        np.add(negvector, negGrad * self.lr, out= negvector);\n",
    "\n",
    "        ## Compute the loss\n",
    "        z = expit(np.dot(ctxtvector, vector))\n",
    "        zNeg = expit(-np.dot(negvector, vector))\n",
    "        self.accLoss -= np.log(z) + np.sum(np.log(zNeg))\n",
    "\n",
    "        ## Update the embeddings\n",
    "        self.centerV[wordId] = vector\n",
    "        self.contxtV[contextId] = contxtGrad\n",
    "        self.contxtV[negativeIds] = negvector\n",
    "\n",
    "\n",
    "    def save(self,path):\n",
    "        \"\"\" We will save the model in a .zip file\"\"\"\n",
    "        import os\n",
    "        from zipfile import ZipFile, ZIP_DEFLATED\n",
    "        from json import dumps\n",
    "\n",
    "        if \".\" in path:\n",
    "            filename_split = path.split('.')\n",
    "            if filename_split[-1] != \"zip\":\n",
    "                filename_split[-1] = \"zip\"\n",
    "                path = \".\".join(filename_split)\n",
    "        else:\n",
    "            path += \"/sg.zip\"\n",
    "\n",
    "        zf = ZipFile(path, mode=\"w\", compression=ZIP_DEFLATED)\n",
    "\n",
    "        ## Save the parameters of the model\n",
    "        model_info = dumps(\n",
    "            {\n",
    "                \"nEmbed\": self.nEmbed,\n",
    "                \"negativeRate\": self.negativeRate,\n",
    "                \"winSize\": self.winSize,\n",
    "                \"minCount\": self.minCount,\n",
    "                \"w2id\": self.w2id,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"lr\": self.lr,\n",
    "            }, indent = 5\n",
    "        )\n",
    "\n",
    "        trainset = dumps(self.trainset, indent=5)\n",
    "        vocab = dumps(self.vocab, indent=5)\n",
    "\n",
    "        zf.writestr(\"model_info.json\", model_info)\n",
    "        zf.writestr(\"trainset.json\", trainset)\n",
    "        zf.writestr(\"vocab.json\", vocab)\n",
    "\n",
    "        # Save embeddings data\n",
    "        np.save(\"centerV.npy\", self.centerV)\n",
    "        zf.write(\"centerV.npy\")\n",
    "        os.remove(\"centerV.npy\")\n",
    "        np.save(\"contxtV.npy\", self.contxtV)\n",
    "        zf.write(\"contxtV.npy\")\n",
    "        os.remove(\"contxtV.npy\")\n",
    "        np.save(\"freq.npy\", self.freq)\n",
    "        zf.write(\"freq.npy\")\n",
    "        os.remove(\"freq.npy\")\n",
    "        np.save(\"loss.npy\", self.loss)\n",
    "        zf.write(\"loss.npy\")\n",
    "        os.remove(\"loss.npy\")\n",
    "\n",
    "        zf.close()\n",
    "\n",
    "\n",
    "\n",
    "    def similarity(self,word1,word2):\n",
    "        \"\"\"\n",
    "        computes similiarity between the two words. unknown words are mapped to one common vector\n",
    "        :param word1:\n",
    "        :param word2:\n",
    "        :return: a float \\in [0,1] indicating the similarity (the higher the more similar)\n",
    "        \"\"\"\n",
    "\n",
    "        word1, word2 = word1.lower(), word2.lower()\n",
    "        if (word1 in self.vocab) and (word2 in self.vocab):\n",
    "            vec1 = self.centerV[self.w2id[word1]]\n",
    "            vec2 = self.centerV[self.w2id[word2]]\n",
    "\n",
    "            # compute the cosine similarity\n",
    "            cos_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "            score = np.clip(cos_similarity, 0, 1) # limit the value between 0 and 1\n",
    "        else:\n",
    "            score = np.random.rand() # assign a random score to the pair if a word is not in the vocabulary\n",
    "\n",
    "        if score <= 1e-4:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        from json import loads\n",
    "        from io import BytesIO\n",
    "        from zipfile import ZipFile\n",
    "\n",
    "        try:\n",
    "            zf = ZipFile(path, \"r\")\n",
    "        except FileNotFoundError:\n",
    "            path = path.split('.')\n",
    "            path[-1] = 'zip'\n",
    "            path = '.'.join(path)\n",
    "            zf = ZipFile(path, \"r\")\n",
    "\n",
    "        model_info = loads(zf.read(\"model_info.json\"))\n",
    "        trainset = loads(zf.read('trainset.json'))\n",
    "        vocab = loads(zf.read('vocab.json'))\n",
    "\n",
    "        sg = SkipGram(trainset, nEmbed=model_info['nEmbed'], negativeRate=model_info['negativeRate'],\n",
    "        winSize=model_info['winSize'], minCount=model_info['minCount'], epochs=model_info['epochs'],\n",
    "        lr=model_info['lr'])\n",
    "\n",
    "        sg.vocab = vocab\n",
    "        sg.centerV = np.load(BytesIO(zf.read('centerV.npy')))\n",
    "        sg.contxtV = np.load(BytesIO(zf.read('contxtV.npy')))\n",
    "        sg.loss = np.load(BytesIO(zf.read('loss.npy'))).tolist()\n",
    "        sg.freq = np.load(BytesIO(zf.read('freq.npy')))\n",
    "\n",
    "        zf.close()\n",
    "\n",
    "        return sg\n",
    "    \n",
    "    #### Additional Step : Debias #####\n",
    "    \n",
    "    # Step 1: Identify the direction of embedding that captures the gender subspace    \n",
    "    def doPCA(self, pairs, num_components = 10):\n",
    "            matrix = []\n",
    "            for a, b in pairs:\n",
    "                    if (a in self.vocab) and (b in self.vocab):\n",
    "                        center = (self.centerV[self.w2id[a]] + self.centerV[self.w2id[b]])/2\n",
    "                        matrix.append(self.centerV[self.w2id[a]] - center)\n",
    "                        matrix.append(self.centerV[self.w2id[a]] - center)\n",
    "                    else: continue\n",
    "            matrix = np.array(matrix)\n",
    "            pca = PCA(n_components = num_components)\n",
    "            pca.fit(matrix)\n",
    "            return pca\n",
    "    \n",
    "    # Step 2: Neutralize - to make sure the gender neutral words are zero in gender subspace\n",
    "    \n",
    "    def debias(self, gender_specific_words, definitional):\n",
    "            gender_direction = self.doPCA(definitional).components_[0]\n",
    "            specific_set = set(gender_specific_words)\n",
    "            for i, w in enumerate(self.vocab):\n",
    "                if w not in specific_set:\n",
    "                    self.centerV[self.w2id[w]] = drop(self.centerV[self.w2id[w]], gender_direction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e1edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('C:/Users/ofir6/Desktop/NLP final/definitional_pairs.json')\n",
    "definitional = json.load(f1)\n",
    "    \n",
    "f2 = open('C:/Users/ofir6/Desktop/NLP final/gender_specific_seed.json')\n",
    "gender_specific_words = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09c6ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'C:/Users/ofir6/Desktop/NLP final/news.en-00024-of-00100'\n",
    "path = 'C:/Users/ofir6/Desktop/NLP final/train.txt'\n",
    "sentences = text2sentences(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74e0310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'minutes', 'could', 'shed', 'light', 'on', 'an', 'internal', 'debate', 'which', 'has', 'been', 'evident', 'in', 'fed', 'officials', 'recent', 'speeches', 'over', 'when', 'to', 'consider', 'raising', 'rates']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22d92622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering the training phase.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch: 1 of 1\n",
      " > training 0 of 168189 - Duration: 0:00:00.027899\n",
      " > training 1000 of 168189 - Duration: 0:00:12.238415\n",
      " > training 2000 of 168189 - Duration: 0:00:24.174419\n",
      " > training 3000 of 168189 - Duration: 0:00:35.722723\n",
      " > training 4000 of 168189 - Duration: 0:00:47.174827\n",
      " > training 5000 of 168189 - Duration: 0:00:59.236450\n",
      " > training 6000 of 168189 - Duration: 0:01:10.502461\n",
      " > training 7000 of 168189 - Duration: 0:01:22.257468\n",
      " > training 8000 of 168189 - Duration: 0:01:34.131396\n",
      " > training 9000 of 168189 - Duration: 0:01:45.599354\n",
      " > training 10000 of 168189 - Duration: 0:01:56.950377\n",
      " > training 11000 of 168189 - Duration: 0:02:12.482544\n",
      " > training 12000 of 168189 - Duration: 0:02:26.412726\n",
      " > training 13000 of 168189 - Duration: 0:02:38.273514\n",
      " > training 14000 of 168189 - Duration: 0:02:49.756621\n",
      " > training 15000 of 168189 - Duration: 0:03:01.321216\n",
      " > training 16000 of 168189 - Duration: 0:03:13.076828\n",
      " > training 17000 of 168189 - Duration: 0:03:25.488298\n",
      " > training 18000 of 168189 - Duration: 0:03:37.351131\n",
      " > training 19000 of 168189 - Duration: 0:03:50.114519\n",
      " > training 20000 of 168189 - Duration: 0:04:02.740240\n",
      " > training 21000 of 168189 - Duration: 0:04:17.494318\n",
      " > training 22000 of 168189 - Duration: 0:04:34.882565\n",
      " > training 23000 of 168189 - Duration: 0:04:52.637859\n",
      " > training 24000 of 168189 - Duration: 0:05:07.024272\n",
      " > training 25000 of 168189 - Duration: 0:05:20.827854\n",
      " > training 26000 of 168189 - Duration: 0:05:33.963417\n",
      " > training 27000 of 168189 - Duration: 0:05:50.610241\n",
      " > training 28000 of 168189 - Duration: 0:06:10.625473\n",
      " > training 29000 of 168189 - Duration: 0:06:27.381478\n",
      " > training 30000 of 168189 - Duration: 0:06:42.782213\n",
      " > training 31000 of 168189 - Duration: 0:06:58.550388\n",
      " > training 32000 of 168189 - Duration: 0:07:13.378625\n",
      " > training 33000 of 168189 - Duration: 0:07:25.925937\n",
      " > training 34000 of 168189 - Duration: 0:07:38.528540\n",
      " > training 35000 of 168189 - Duration: 0:07:54.633242\n",
      " > training 36000 of 168189 - Duration: 0:08:09.865057\n",
      " > training 37000 of 168189 - Duration: 0:08:28.155207\n",
      " > training 38000 of 168189 - Duration: 0:08:45.913244\n",
      " > training 39000 of 168189 - Duration: 0:09:01.288316\n",
      " > training 40000 of 168189 - Duration: 0:09:14.732086\n",
      " > training 41000 of 168189 - Duration: 0:09:27.854184\n",
      " > training 42000 of 168189 - Duration: 0:09:40.646184\n",
      " > training 43000 of 168189 - Duration: 0:09:55.476376\n",
      " > training 44000 of 168189 - Duration: 0:10:11.140705\n",
      " > training 45000 of 168189 - Duration: 0:10:27.270356\n",
      " > training 46000 of 168189 - Duration: 0:10:42.914581\n",
      " > training 47000 of 168189 - Duration: 0:10:57.715162\n",
      " > training 48000 of 168189 - Duration: 0:11:12.260292\n",
      " > training 49000 of 168189 - Duration: 0:11:26.666057\n",
      " > training 50000 of 168189 - Duration: 0:11:40.558565\n",
      " > training 51000 of 168189 - Duration: 0:11:55.243194\n",
      " > training 52000 of 168189 - Duration: 0:12:09.033550\n",
      " > training 53000 of 168189 - Duration: 0:12:23.274730\n",
      " > training 54000 of 168189 - Duration: 0:12:37.624253\n",
      " > training 55000 of 168189 - Duration: 0:12:51.789875\n",
      " > training 56000 of 168189 - Duration: 0:13:07.652898\n",
      " > training 57000 of 168189 - Duration: 0:13:21.858550\n",
      " > training 58000 of 168189 - Duration: 0:13:34.694122\n",
      " > training 59000 of 168189 - Duration: 0:13:49.302024\n",
      " > training 60000 of 168189 - Duration: 0:14:02.873675\n",
      " > training 61000 of 168189 - Duration: 0:14:15.987612\n",
      " > training 62000 of 168189 - Duration: 0:14:29.734630\n",
      " > training 63000 of 168189 - Duration: 0:14:42.765813\n",
      " > training 64000 of 168189 - Duration: 0:14:56.293682\n",
      " > training 65000 of 168189 - Duration: 0:15:09.949992\n",
      " > training 66000 of 168189 - Duration: 0:15:23.217064\n",
      " > training 67000 of 168189 - Duration: 0:15:36.176390\n",
      " > training 68000 of 168189 - Duration: 0:15:49.477120\n",
      " > training 69000 of 168189 - Duration: 0:16:02.836108\n",
      " > training 70000 of 168189 - Duration: 0:16:15.899579\n",
      " > training 71000 of 168189 - Duration: 0:16:28.855875\n",
      " > training 72000 of 168189 - Duration: 0:16:41.665873\n",
      " > training 73000 of 168189 - Duration: 0:16:54.825453\n",
      " > training 74000 of 168189 - Duration: 0:17:07.898502\n",
      " > training 75000 of 168189 - Duration: 0:17:21.159739\n",
      " > training 76000 of 168189 - Duration: 0:17:34.276386\n",
      " > training 77000 of 168189 - Duration: 0:17:48.359233\n",
      " > training 78000 of 168189 - Duration: 0:18:01.473248\n",
      " > training 79000 of 168189 - Duration: 0:18:14.967094\n",
      " > training 80000 of 168189 - Duration: 0:18:28.090455\n",
      " > training 81000 of 168189 - Duration: 0:18:42.677059\n",
      " > training 82000 of 168189 - Duration: 0:18:57.023446\n",
      " > training 83000 of 168189 - Duration: 0:19:11.438591\n",
      " > training 84000 of 168189 - Duration: 0:19:26.468943\n",
      " > training 85000 of 168189 - Duration: 0:19:43.403074\n",
      " > training 86000 of 168189 - Duration: 0:19:57.996680\n",
      " > training 87000 of 168189 - Duration: 0:20:13.253587\n",
      " > training 88000 of 168189 - Duration: 0:20:27.571297\n",
      " > training 89000 of 168189 - Duration: 0:20:42.813756\n",
      " > training 90000 of 168189 - Duration: 0:20:57.017321\n",
      " > training 91000 of 168189 - Duration: 0:21:11.206908\n",
      " > training 92000 of 168189 - Duration: 0:21:24.001247\n",
      " > training 93000 of 168189 - Duration: 0:21:37.264427\n",
      " > training 94000 of 168189 - Duration: 0:21:50.308483\n",
      " > training 95000 of 168189 - Duration: 0:22:03.673542\n",
      " > training 96000 of 168189 - Duration: 0:22:16.722542\n",
      " > training 97000 of 168189 - Duration: 0:22:30.135873\n",
      " > training 98000 of 168189 - Duration: 0:22:44.843295\n",
      " > training 99000 of 168189 - Duration: 0:22:58.483611\n",
      " > training 100000 of 168189 - Duration: 0:23:11.415144\n",
      " > training 101000 of 168189 - Duration: 0:23:26.115131\n",
      " > training 102000 of 168189 - Duration: 0:23:39.244146\n",
      " > training 103000 of 168189 - Duration: 0:23:52.857870\n",
      " > training 104000 of 168189 - Duration: 0:24:05.834200\n",
      " > training 105000 of 168189 - Duration: 0:24:18.915927\n",
      " > training 106000 of 168189 - Duration: 0:24:32.622018\n",
      " > training 107000 of 168189 - Duration: 0:24:46.518319\n",
      " > training 108000 of 168189 - Duration: 0:25:00.922263\n",
      " > training 109000 of 168189 - Duration: 0:25:18.285547\n",
      " > training 110000 of 168189 - Duration: 0:25:33.770773\n",
      " > training 111000 of 168189 - Duration: 0:25:48.304693\n",
      " > training 112000 of 168189 - Duration: 0:26:06.395523\n",
      " > training 113000 of 168189 - Duration: 0:26:23.397024\n",
      " > training 114000 of 168189 - Duration: 0:26:41.492975\n",
      " > training 115000 of 168189 - Duration: 0:26:56.752213\n",
      " > training 116000 of 168189 - Duration: 0:27:11.825051\n",
      " > training 117000 of 168189 - Duration: 0:27:25.623429\n",
      " > training 118000 of 168189 - Duration: 0:27:38.499382\n",
      " > training 119000 of 168189 - Duration: 0:27:50.713384\n",
      " > training 120000 of 168189 - Duration: 0:28:03.331502\n",
      " > training 121000 of 168189 - Duration: 0:28:16.042563\n",
      " > training 122000 of 168189 - Duration: 0:28:28.639553\n",
      " > training 123000 of 168189 - Duration: 0:28:40.930485\n",
      " > training 124000 of 168189 - Duration: 0:28:56.555114\n",
      " > training 125000 of 168189 - Duration: 0:29:14.181428\n",
      " > training 126000 of 168189 - Duration: 0:29:29.612852\n",
      " > training 127000 of 168189 - Duration: 0:29:45.433592\n",
      " > training 128000 of 168189 - Duration: 0:30:02.275714\n",
      " > training 129000 of 168189 - Duration: 0:30:20.242051\n",
      " > training 130000 of 168189 - Duration: 0:30:36.131744\n",
      " > training 131000 of 168189 - Duration: 0:30:55.910421\n",
      " > training 132000 of 168189 - Duration: 0:31:11.897974\n",
      " > training 133000 of 168189 - Duration: 0:31:28.771802\n",
      " > training 134000 of 168189 - Duration: 0:31:44.361188\n",
      " > training 135000 of 168189 - Duration: 0:31:58.241035\n",
      " > training 136000 of 168189 - Duration: 0:32:12.759900\n",
      " > training 137000 of 168189 - Duration: 0:32:29.634599\n",
      " > training 138000 of 168189 - Duration: 0:32:49.717697\n",
      " > training 139000 of 168189 - Duration: 0:33:07.711423\n",
      " > training 140000 of 168189 - Duration: 0:33:23.209683\n",
      " > training 141000 of 168189 - Duration: 0:33:39.289280\n",
      " > training 142000 of 168189 - Duration: 0:33:56.120703\n",
      " > training 143000 of 168189 - Duration: 0:34:11.963787\n",
      " > training 144000 of 168189 - Duration: 0:34:27.707597\n",
      " > training 145000 of 168189 - Duration: 0:34:42.714070\n",
      " > training 146000 of 168189 - Duration: 0:34:58.023370\n",
      " > training 147000 of 168189 - Duration: 0:35:13.584239\n",
      " > training 148000 of 168189 - Duration: 0:35:30.764929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 149000 of 168189 - Duration: 0:35:46.834915\n",
      " > training 150000 of 168189 - Duration: 0:36:06.173924\n",
      " > training 151000 of 168189 - Duration: 0:36:21.728134\n",
      " > training 152000 of 168189 - Duration: 0:36:34.461524\n",
      " > training 153000 of 168189 - Duration: 0:36:47.666666\n",
      " > training 154000 of 168189 - Duration: 0:37:00.252333\n",
      " > training 155000 of 168189 - Duration: 0:37:13.026131\n",
      " > training 156000 of 168189 - Duration: 0:37:25.746162\n",
      " > training 157000 of 168189 - Duration: 0:37:38.203582\n",
      " > training 158000 of 168189 - Duration: 0:37:50.326465\n",
      " > training 159000 of 168189 - Duration: 0:38:02.662557\n",
      " > training 160000 of 168189 - Duration: 0:38:15.181530\n",
      " > training 161000 of 168189 - Duration: 0:38:27.737948\n",
      " > training 162000 of 168189 - Duration: 0:38:40.261831\n",
      " > training 163000 of 168189 - Duration: 0:38:52.563651\n",
      " > training 164000 of 168189 - Duration: 0:39:05.018595\n",
      " > training 165000 of 168189 - Duration: 0:39:18.582812\n",
      " > training 166000 of 168189 - Duration: 0:39:31.189848\n",
      " > training 167000 of 168189 - Duration: 0:39:43.135942\n",
      " > training 168000 of 168189 - Duration: 0:39:56.146582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [39:58<00:00, 2398.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 85.38261956925695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/ofir6/Desktop/NLP final/train.txt'\n",
    "sentences = text2sentences(path)\n",
    "sg = SkipGram(sentences)\n",
    "sg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cbb0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below line to run the debias before saving the embedding matrix\n",
    "sg.debias(gender_specific_words, definitional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b453e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'C:/Users/ofir6/Desktop/NLP final/'\n",
    "sg.save(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c7145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55060956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/ofir6/Desktop/NLP final/SimLex-999.txt', sep ='\\t', engine='python')\n",
    "pairs = zip(data['word1'],data['word2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bed583b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = SkipGram.load('C:/Users/ofir6/Desktop/NLP final/sg.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "041fdf8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old new 0.53\n",
      "smart intelligent 0.1\n",
      "hard difficult 0.45\n",
      "happy cheerful 0.12\n",
      "hard easy 0.44\n",
      "fast rapid 0.4\n",
      "happy glad 0.11\n",
      "short long 0.66\n",
      "stupid dumb 0.06\n",
      "weird strange 0\n",
      "wide narrow 0.21\n",
      "bad awful 0.2\n",
      "easy difficult 0.45\n",
      "bad terrible 0.21\n",
      "hard simple 0.32\n",
      "smart dumb 0.13\n",
      "insane crazy 0.26\n",
      "happy mad 0.15\n",
      "large huge 0.38\n",
      "hard tough 0.45\n",
      "new fresh 0.39\n",
      "sharp dull 0.06\n",
      "quick rapid 0.33\n",
      "dumb foolish 0.2\n",
      "wonderful terrific 0.29\n",
      "strange odd 0.17\n",
      "happy angry 0.23\n",
      "narrow broad 0.28\n",
      "simple easy 0.27\n",
      "old fresh 0.4\n",
      "apparent obvious 0.16\n",
      "inexpensive cheap 0.14\n",
      "nice generous 0.27\n",
      "weird normal 0.16\n",
      "weird odd 0.01\n",
      "bad immoral 0.19\n",
      "sad funny 0.17\n",
      "wonderful great 0.15\n",
      "guilty ashamed 0.16\n",
      "beautiful wonderful 0.24\n",
      "confident sure 0.15\n",
      "dumb dense 0.18\n",
      "large big 0.54\n",
      "nice cruel 0.25\n",
      "impatient anxious 0.1\n",
      "big broad 0.34\n",
      "strong proud 0.36\n",
      "unnecessary necessary 0.19\n",
      "restless young 0.16\n",
      "dumb intelligent 0.12\n",
      "bad great 0.41\n",
      "difficult simple 0.41\n",
      "necessary important 0.43\n",
      "bad terrific 0.12\n",
      "mad glad 0.01\n",
      "honest guilty 0.13\n",
      "easy tough 0.32\n",
      "easy flexible 0.19\n",
      "certain sure 0.48\n",
      "essential necessary 0.19\n",
      "different normal 0.32\n",
      "sly clever 0.27\n",
      "crucial important 0.41\n",
      "harsh cruel 0.33\n",
      "childish foolish 0.11\n",
      "scarce rare 0.2\n",
      "friendly generous 0.13\n",
      "fragile frigid 0.04\n",
      "long narrow 0.17\n",
      "big heavy 0.44\n",
      "rough frigid 0.09\n",
      "bizarre strange 0.19\n",
      "illegal immoral 0.07\n",
      "bad guilty 0.44\n",
      "modern ancient 0.22\n",
      "new ancient 0.28\n",
      "dull funny 0.18\n",
      "happy young 0.4\n",
      "easy big 0.45\n",
      "great awful 0.27\n",
      "tiny huge 0.4\n",
      "polite proper 0.28\n",
      "modest ashamed 0.05\n",
      "exotic rare 0.12\n",
      "dumb clever 0.16\n",
      "delightful wonderful 0.05\n",
      "noticeable obvious 0.02\n",
      "afraid anxious 0.23\n",
      "formal proper 0.25\n",
      "dreary dull 0.03\n",
      "delightful cheerful 0.05\n",
      "unhappy mad 0\n",
      "sad terrible 0.08\n",
      "sick crazy 0.25\n",
      "violent angry 0.07\n",
      "laden heavy 0.23\n",
      "dirty cheap 0.18\n",
      "elastic flexible 0\n",
      "hard dense 0.11\n",
      "recent new 0.59\n",
      "bold proud 0.26\n",
      "sly strange 0\n",
      "strange sly 0\n",
      "dumb rare 0.21\n",
      "sly tough 0\n",
      "terrific mad 0.08\n",
      "modest flexible 0.04\n",
      "fresh wide 0.32\n",
      "huge dumb 0.19\n",
      "large flexible 0.07\n",
      "dirty narrow 0.15\n",
      "wife husband 0.47\n",
      "book text 0.32\n",
      "groom bride 0.11\n",
      "night day 0.5\n",
      "south north 0.47\n",
      "plane airport 0.39\n",
      "uncle aunt 0.18\n",
      "horse mare 0.19\n",
      "bottom top 0.3\n",
      "friend buddy 0.01\n",
      "student pupil 0.19\n",
      "world globe 0.25\n",
      "leg arm 0.2\n",
      "plane jet 0.24\n",
      "woman man 0.53\n",
      "horse colt 0.11\n",
      "actress actor 0.3\n",
      "teacher instructor 0.02\n",
      "movie film 0.54\n",
      "bird hawk 0.17\n",
      "word dictionary 0\n",
      "money salary 0.33\n",
      "dog cat 0.19\n",
      "area region 0.41\n",
      "navy army 0.29\n",
      "book literature 0.21\n",
      "clothes closet 0.05\n",
      "sunset sunrise 0.19\n",
      "child adult 0.27\n",
      "cow cattle 0.14\n",
      "book story 0.46\n",
      "winter summer 0.3\n",
      "taxi cab 0.15\n",
      "tree maple 0.13\n",
      "bed bedroom 0.1\n",
      "roof ceiling 0.07\n",
      "disease infection 0.18\n",
      "arm shoulder 0.24\n",
      "sheep lamb 0.24\n",
      "lady gentleman 0.04\n",
      "boat anchor 0.22\n",
      "priest monk 0.26\n",
      "toe finger 0.2\n",
      "river stream 0.29\n",
      "anger fury 0.2\n",
      "date calendar 0.3\n",
      "sea ocean 0.21\n",
      "second minute 0.46\n",
      "hand thumb 0.14\n",
      "wood log 0.13\n",
      "mud dirt 0.11\n",
      "hallway corridor 0.16\n",
      "way manner 0.27\n",
      "mouse cat 0.16\n",
      "cop sheriff 0.18\n",
      "death burial 0.14\n",
      "music melody 0.17\n",
      "beer alcohol 0.3\n",
      "mouth lip 0.09\n",
      "storm hurricane 0.23\n",
      "tax income 0.5\n",
      "flower violet 0.69\n",
      "paper cardboard 0.31\n",
      "floor ceiling 0.34\n",
      "beach seashore 0.99\n",
      "rod curtain 0.26\n",
      "hound fox 0.29\n",
      "street alley 0.23\n",
      "boat deck 0.13\n",
      "car horn 0.22\n",
      "friend guest 0.29\n",
      "employer employee 0.2\n",
      "hand wrist 0.15\n",
      "ball cannon 0.19\n",
      "alcohol brandy 0.47\n",
      "victory triumph 0.22\n",
      "telephone booth 0.2\n",
      "door doorway 0.02\n",
      "motel inn 0.08\n",
      "clothes cloth 0.11\n",
      "steak meat 0.08\n",
      "nail thumb 0.19\n",
      "band orchestra 0.11\n",
      "book bible 0.21\n",
      "business industry 0.47\n",
      "winter season 0.27\n",
      "decade century 0.28\n",
      "alcohol gin 0.46\n",
      "hat coat 0.12\n",
      "window door 0.3\n",
      "arm wrist 0.12\n",
      "house apartment 0.2\n",
      "glass crystal 0\n",
      "wine brandy 0.7\n",
      "creator maker 0.23\n",
      "dinner breakfast 0.22\n",
      "arm muscle 0.29\n",
      "bubble suds 0.27\n",
      "bread flour 0.11\n",
      "death tragedy 0.25\n",
      "absence presence 0.08\n",
      "gun cannon 0.06\n",
      "grass blade 0.08\n",
      "ball basket 0.3\n",
      "hose garden 0.17\n",
      "boy kid 0.13\n",
      "church choir 0.29\n",
      "clothes drawer 0.09\n",
      "tower bell 0.34\n",
      "father parent 0.23\n",
      "school grade 0.33\n",
      "parent adult 0.23\n",
      "bar jail 0.22\n",
      "car highway 0.42\n",
      "dictionary definition 0.35\n",
      "door cellar 0.15\n",
      "army legion 0.27\n",
      "metal aluminum 0.13\n",
      "chair bench 0.25\n",
      "cloud fog 0.13\n",
      "boy son 0.45\n",
      "water ice 0.55\n",
      "bed blanket 0.12\n",
      "attorney lawyer 0.42\n",
      "area zone 0.42\n",
      "business company 0.39\n",
      "clothes fabric 0.05\n",
      "sweater jacket 0.27\n",
      "money capital 0.52\n",
      "hand foot 0.25\n",
      "alcohol cocktail 0.18\n",
      "yard inch 0.29\n",
      "molecule atom 0.1\n",
      "lens camera 0.16\n",
      "meal dinner 0.16\n",
      "eye tear 0.16\n",
      "god devil 0.22\n",
      "loop belt 0.14\n",
      "rat mouse 0.03\n",
      "motor engine 0.04\n",
      "car cab 0.42\n",
      "cat lion 0.18\n",
      "size magnitude 0.2\n",
      "reality fantasy 0.34\n",
      "door gate 0.17\n",
      "cat pet 0.34\n",
      "tin aluminum 0.15\n",
      "bone jaw 0.14\n",
      "cereal wheat 0.06\n",
      "house key 0.39\n",
      "blood flesh 0.21\n",
      "door corridor 0.23\n",
      "god spirit 0.35\n",
      "capability competence 0.13\n",
      "abundance plenty 0.29\n",
      "sofa chair 0.33\n",
      "wall brick 0.15\n",
      "horn drum 0.17\n",
      "organ liver 0.07\n",
      "strength might 0.29\n",
      "phrase word 0.42\n",
      "band parade 0.06\n",
      "stomach waist 0.3\n",
      "cloud storm 0.02\n",
      "joy pride 0.16\n",
      "noise rattle 0.15\n",
      "rain mist 0.07\n",
      "beer beverage 0.22\n",
      "man uncle 0.15\n",
      "apple juice 0.19\n",
      "intelligence logic 0.34\n",
      "communication language 0.19\n",
      "mink fur 0.77\n",
      "mob crowd 0.14\n",
      "shore coast 0.3\n",
      "wire cord 0.28\n",
      "bird turkey 0.34\n",
      "bed crib 0.24\n",
      "competence ability 0.22\n",
      "cloud haze 0.11\n",
      "supper meal 0.07\n",
      "bar cage 0.02\n",
      "water salt 0.29\n",
      "sense intuition 0.36\n",
      "situation condition 0.34\n",
      "crime theft 0.35\n",
      "style fashion 0.3\n",
      "boundary border 0.17\n",
      "arm body 0.29\n",
      "boat car 0.23\n",
      "sandwich lunch 0.18\n",
      "bride princess 0.21\n",
      "heroine hero 0.12\n",
      "car gauge 0.21\n",
      "insect bee 0.17\n",
      "crib cradle 0.14\n",
      "animal person 0.41\n",
      "marijuana herb 0.05\n",
      "bed hospital 0.28\n",
      "cheek tongue 0.04\n",
      "disc computer 0.13\n",
      "curve angle 0\n",
      "grass moss 0.26\n",
      "school law 0.53\n",
      "foot head 0.37\n",
      "mother guardian 0.29\n",
      "orthodontist dentist 0.56\n",
      "alcohol whiskey 0.08\n",
      "mouth tooth 0.15\n",
      "breakfast bacon 0.2\n",
      "bathroom bedroom 0.2\n",
      "plate bowl 0.21\n",
      "meat bacon 0.2\n",
      "air helium 0.3\n",
      "worker employer 0.1\n",
      "body chest 0.32\n",
      "son father 0.57\n",
      "heart surgery 0.39\n",
      "woman secretary 0.44\n",
      "man father 0.45\n",
      "beach island 0.35\n",
      "story topic 0.22\n",
      "game fun 0.22\n",
      "weekend week 0.42\n",
      "couple pair 0.25\n",
      "woman wife 0.41\n",
      "sheep cattle 0.18\n",
      "purse bag 0.23\n",
      "ceiling cathedral 0.15\n",
      "bean coffee 0\n",
      "wood paper 0.38\n",
      "top side 0.5\n",
      "crime fraud 0.37\n",
      "pain harm 0.36\n",
      "lover companion 0.06\n",
      "evening dusk 0.15\n",
      "father daughter 0.48\n",
      "wine liquor 0.24\n",
      "cow goat 0.07\n",
      "belief opinion 0.12\n",
      "reality illusion 0.34\n",
      "pact agreement 0.11\n",
      "wealth poverty 0.32\n",
      "accident emergency 0.4\n",
      "battle conquest 0.28\n",
      "friend teacher 0.29\n",
      "illness infection 0.12\n",
      "game trick 0.11\n",
      "brother son 0.32\n",
      "aunt nephew 0.17\n",
      "worker mechanic 0.24\n",
      "doctor orthodontist 0.38\n",
      "oak maple 0.14\n",
      "bee queen 0.08\n",
      "car bicycle 0.16\n",
      "goal quest 0.31\n",
      "august month 0.56\n",
      "army squad 0.44\n",
      "cloud weather 0.2\n",
      "physician doctor 0.22\n",
      "canyon valley 0.31\n",
      "river valley 0.2\n",
      "sun sky 0.16\n",
      "target arrow 0.12\n",
      "chocolate pie 0.18\n",
      "circumstance situation 0.31\n",
      "opinion choice 0.29\n",
      "rhythm melody 0.07\n",
      "gut nerve 0.32\n",
      "day dawn 0.14\n",
      "cattle beef 0.15\n",
      "doctor professor 0.46\n",
      "arm vein 0.05\n",
      "room bath 0.27\n",
      "corporation business 0.29\n",
      "fun football 0.37\n",
      "hill cliff 0.31\n",
      "bone ankle 0.19\n",
      "apple candy 0.11\n",
      "helper maid 0.78\n",
      "leader manager 0.29\n",
      "lemon tea 0.15\n",
      "bee ant 0.24\n",
      "basketball baseball 0.11\n",
      "rice bean 0.3\n",
      "bed furniture 0.22\n",
      "emotion passion 0.19\n",
      "anarchy chaos 0.18\n",
      "crime violation 0.34\n",
      "machine engine 0.13\n",
      "beach sea 0.28\n",
      "alley bowl 0.35\n",
      "jar bottle 0.12\n",
      "strength capability 0.18\n",
      "seed mustard 0.33\n",
      "guitar drum 0\n",
      "opinion idea 0.48\n",
      "north west 0.45\n",
      "diet salad 0.3\n",
      "mother wife 0.49\n",
      "dad mother 0.23\n",
      "captain sailor 0.1\n",
      "meter yard 0.25\n",
      "beer champagne 0.25\n",
      "motor boat 0.19\n",
      "card bridge 0.3\n",
      "science psychology 0.24\n",
      "sinner saint 0.23\n",
      "destruction construction 0.3\n",
      "crowd bunch 0.23\n",
      "beach reef 0.17\n",
      "man child 0.54\n",
      "bread cheese 0.24\n",
      "champion winner 0.28\n",
      "celebration ceremony 0.31\n",
      "menu order 0.21\n",
      "king princess 0.24\n",
      "wealth prestige 0.14\n",
      "endurance strength 0.26\n",
      "danger threat 0.19\n",
      "god priest 0.31\n",
      "men fraternity 0.13\n",
      "buddy companion 0.11\n",
      "teacher helper 0.65\n",
      "body stomach 0.22\n",
      "tongue throat 0.14\n",
      "house carpet 0.12\n",
      "intelligence skill 0.09\n",
      "journey conquest 0.15\n",
      "god prey 0.21\n",
      "brother soul 0.11\n",
      "adversary opponent 0.09\n",
      "death catastrophe 0.19\n",
      "monster demon 0.06\n",
      "day morning 0.38\n",
      "man victor 0.27\n",
      "friend guy 0.38\n",
      "song story 0.27\n",
      "ray sunshine 0.25\n",
      "guy stud 0.11\n",
      "chicken rice 0.17\n",
      "box elevator 0.31\n",
      "butter potato 0.0\n",
      "apartment furniture 0.27\n",
      "lake swamp 0.24\n",
      "salad vinegar 0.26\n",
      "flower bulb 0.13\n",
      "cloud mist 0.02\n",
      "driver pilot 0.43\n",
      "sugar honey 0.21\n",
      "body shoulder 0.51\n",
      "idea image 0.41\n",
      "father brother 0.45\n",
      "moon planet 0.02\n",
      "ball costume 0.24\n",
      "rail fence 0.06\n",
      "room bed 0.36\n",
      "flower bush 0.05\n",
      "bone knee 0.17\n",
      "arm knee 0.2\n",
      "bottom side 0.17\n",
      "vessel vein 0.17\n",
      "cat rabbit 0.1\n",
      "meat sandwich 0.27\n",
      "belief concept 0.09\n",
      "intelligence insight 0.34\n",
      "attention interest 0.27\n",
      "attitude confidence 0.3\n",
      "right justice 0.43\n",
      "argument agreement 0.31\n",
      "depth magnitude 0.14\n",
      "medium news 0.22\n",
      "winner candidate 0.43\n",
      "birthday date 0.38\n",
      "fee payment 0.4\n",
      "bible hymn 0.74\n",
      "exit doorway 0.12\n",
      "man sentry 0.04\n",
      "aisle hall 0.24\n",
      "whiskey gin 0.2\n",
      "blood marrow 0.29\n",
      "oil mink 0.08\n",
      "floor deck 0.18\n",
      "roof floor 0.21\n",
      "door floor 0.33\n",
      "shoulder head 0.28\n",
      "wagon carriage 0.14\n",
      "car carriage 0.14\n",
      "elbow ankle 0.19\n",
      "wealth fame 0.28\n",
      "sorrow shame 0.2\n",
      "administration management 0.53\n",
      "communication conversation 0.19\n",
      "pollution atmosphere 0.28\n",
      "anatomy biology 0.24\n",
      "college profession 0.19\n",
      "book topic 0.24\n",
      "formula equation 0.02\n",
      "book information 0.38\n",
      "boy partner 0.34\n",
      "sky universe 0.15\n",
      "population people 0.44\n",
      "college class 0.42\n",
      "chief mayor 0.29\n",
      "rabbi minister 0.16\n",
      "meter inch 0.26\n",
      "polyester cotton 0.91\n",
      "lawyer banker 0.11\n",
      "violin instrument 0.13\n",
      "camp cabin 0.23\n",
      "pot appliance 0.07\n",
      "linen fabric 0.16\n",
      "whiskey champagne 0.31\n",
      "girl child 0.42\n",
      "cottage cabin 0.13\n",
      "bird hen 0.5\n",
      "racket noise 0.07\n",
      "sunset evening 0.01\n",
      "drizzle rain 0\n",
      "adult baby 0.21\n",
      "charcoal coal 0.7\n",
      "body spine 0.39\n",
      "head nail 0.05\n",
      "log timber 0.2\n",
      "spoon cup 0.24\n",
      "body nerve 0.27\n",
      "man husband 0.52\n",
      "bone neck 0.2\n",
      "frustration anger 0.08\n",
      "river sea 0.33\n",
      "task job 0.34\n",
      "club society 0.41\n",
      "reflection image 0.28\n",
      "prince king 0.46\n",
      "snow weather 0.36\n",
      "people party 0.48\n",
      "boy brother 0.37\n",
      "root grass 0.35\n",
      "brow eye 0.07\n",
      "money pearl 0.19\n",
      "money diamond 0.21\n",
      "vehicle bus 0.44\n",
      "cab bus 0.16\n",
      "house barn 0.25\n",
      "finger palm 0.19\n",
      "car bridge 0.23\n",
      "effort difficulty 0.26\n",
      "fact insight 0.16\n",
      "job management 0.45\n",
      "cancer sickness 0.21\n",
      "word newspaper 0.29\n",
      "composer writer 0.19\n",
      "actor singer 0.41\n",
      "shelter hut 0.05\n",
      "bathroom kitchen 0.06\n",
      "cabin hut 0\n",
      "door kitchen 0.1\n",
      "value belief 0.21\n",
      "wisdom intelligence 0.29\n",
      "ignorance intelligence 0.1\n",
      "happiness luck 0.19\n",
      "idea scheme 0.36\n",
      "mood emotion 0.24\n",
      "happiness peace 0.22\n",
      "despair misery 0.15\n",
      "logic arithmetic 0.1\n",
      "denial confession 0.31\n",
      "argument criticism 0.42\n",
      "aggression hostility 0.27\n",
      "hysteria confusion 0.07\n",
      "chemistry theory 0.11\n",
      "trial verdict 0.37\n",
      "comfort safety 0.17\n",
      "confidence self 0.21\n",
      "vision perception 0.04\n",
      "era decade 0.34\n",
      "biography fiction 0.07\n",
      "discussion argument 0.29\n",
      "code symbol 0.1\n",
      "danger disease 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident catastrophe 0.14\n",
      "journey trip 0.39\n",
      "activity movement 0.45\n",
      "gossip news 0.05\n",
      "father god 0.32\n",
      "action course 0.5\n",
      "fever illness 0.03\n",
      "aviation flight 0.28\n",
      "game action 0.42\n",
      "molecule air 0.19\n",
      "home state 0.48\n",
      "word literature 0.18\n",
      "adult guardian 0.17\n",
      "newspaper information 0.57\n",
      "communication television 0.22\n",
      "cousin uncle 0.22\n",
      "author reader 0.27\n",
      "guy partner 0.33\n",
      "area corner 0.43\n",
      "ballad song 0.1\n",
      "wall decoration 0.1\n",
      "word page 0.42\n",
      "nurse scientist 0.07\n",
      "politician president 0.31\n",
      "president mayor 0.24\n",
      "book essay 0.09\n",
      "man warrior 0.15\n",
      "article journal 0.45\n",
      "breakfast supper 0.11\n",
      "crowd parade 0.23\n",
      "aisle hallway 0.11\n",
      "teacher rabbi 0.1\n",
      "hip lip 0.11\n",
      "book article 0.41\n",
      "room cell 0.39\n",
      "box booth 0.28\n",
      "daughter kid 0.34\n",
      "limb leg 0.2\n",
      "liver lung 0.07\n",
      "classroom hallway 0.14\n",
      "mountain ledge 0.14\n",
      "car elevator 0.11\n",
      "bed couch 0.29\n",
      "clothes button 0.01\n",
      "clothes coat 0.13\n",
      "kidney organ 0.17\n",
      "apple sauce 0.07\n",
      "chicken steak 0.09\n",
      "car hose 0.1\n",
      "tobacco cigarette 0.11\n",
      "student professor 0.55\n",
      "baby daughter 0.45\n",
      "pipe cigar 0.29\n",
      "milk juice 0.29\n",
      "box cigar 0.12\n",
      "apartment hotel 0.31\n",
      "cup cone 0.18\n",
      "horse ox 0.51\n",
      "throat nose 0.01\n",
      "bone teeth 0.24\n",
      "bone elbow 0.27\n",
      "bacon bean 0.17\n",
      "cup jar 0.11\n",
      "proof fact 0.15\n",
      "appointment engagement 0.09\n",
      "birthday year 0.31\n",
      "word clue 0.11\n",
      "author creator 0\n",
      "atom carbon 0.01\n",
      "archbishop bishop 0.16\n",
      "letter paragraph 0.34\n",
      "page paragraph 0.19\n",
      "steeple chapel 0.78\n",
      "muscle bone 0.27\n",
      "muscle tongue 0.05\n",
      "boy soldier 0.32\n",
      "belly abdomen 0.21\n",
      "guy girl 0.39\n",
      "bed chair 0.39\n",
      "clothes jacket 0.32\n",
      "gun knife 0.28\n",
      "tin metal 0.2\n",
      "bottle container 0.12\n",
      "hen turkey 0.69\n",
      "meat bread 0.25\n",
      "arm bone 0.14\n",
      "neck spine 0.17\n",
      "apple lemon 0.18\n",
      "agony grief 0.07\n",
      "assignment task 0.13\n",
      "night dawn 0.26\n",
      "dinner soup 0.17\n",
      "calf bull 0.22\n",
      "snow storm 0.34\n",
      "nail hand 0.15\n",
      "dog horse 0.15\n",
      "arm neck 0.12\n",
      "ball glove 0.18\n",
      "flu fever 0.22\n",
      "fee salary 0.29\n",
      "nerve brain 0.17\n",
      "beast animal 0.22\n",
      "dinner chicken 0.3\n",
      "girl maid 0.18\n",
      "child boy 0.46\n",
      "alcohol wine 0.42\n",
      "nose mouth 0.09\n",
      "street car 0.49\n",
      "bell door 0.14\n",
      "box hat 0.18\n",
      "belief impression 0.18\n",
      "bias opinion 0.13\n",
      "attention awareness 0.23\n",
      "anger mood 0.13\n",
      "elegance style 0.19\n",
      "beauty age 0.27\n",
      "book theme 0.28\n",
      "friend mother 0.39\n",
      "vitamin iron 0.28\n",
      "car factory 0.24\n",
      "pact condition 0.27\n",
      "chapter choice 0.37\n",
      "arithmetic rhythm 0.15\n",
      "winner presence 0.28\n",
      "belief flower 0.16\n",
      "winner goal 0.5\n",
      "trick size 0.2\n",
      "choice vein 0.08\n",
      "hymn conquest 0.2\n",
      "endurance band 0.28\n",
      "jail choice 0.36\n",
      "condition boy 0.4\n",
      "flower endurance 0.13\n",
      "hole agreement 0.19\n",
      "doctor temper 0.03\n",
      "fraternity door 0.01\n",
      "task woman 0.41\n",
      "fraternity baseball 0.2\n",
      "cent size 0.48\n",
      "presence door 0.28\n",
      "mouse management 0.21\n",
      "task highway 0.2\n",
      "liquor century 0.09\n",
      "task straw 0.12\n",
      "island task 0.39\n",
      "night chapter 0.21\n",
      "pollution president 0.26\n",
      "gun trick 0.28\n",
      "bath trick 0.23\n",
      "diet apple 0.42\n",
      "cent wife 0.31\n",
      "chapter tail 0.34\n",
      "course stomach 0.21\n",
      "hymn straw 0.73\n",
      "dentist colonel 0.15\n",
      "wife straw 0.15\n",
      "hole wife 0.22\n",
      "pupil president 0.17\n",
      "bath wife 0.23\n",
      "people cent 0.39\n",
      "formula log 0.19\n",
      "woman fur 0.11\n",
      "apple sunshine 0.2\n",
      "gun dawn 0.25\n",
      "meal waist 0.12\n",
      "camera president 0.29\n",
      "liquor band 0.12\n",
      "stomach vein 0.03\n",
      "gun fur 0.26\n",
      "couch baseball 0.19\n",
      "worker camera 0.27\n",
      "deck mouse 0.09\n",
      "rice boy 0.3\n",
      "people gun 0.39\n",
      "cliff tail 0.23\n",
      "ankle window 0.27\n",
      "princess island 0.27\n",
      "container mouse 0.11\n",
      "wagon container 0.11\n",
      "people balloon 0.25\n",
      "dollar people 0.36\n",
      "bath balloon 0.24\n",
      "stomach bedroom 0.08\n",
      "bicycle bedroom 0.15\n",
      "log bath 0.35\n",
      "bowl tail 0.17\n",
      "go come 0.46\n",
      "take steal 0.21\n",
      "listen hear 0.19\n",
      "think rationalize 0.9\n",
      "occur happen 0.29\n",
      "vanish disappear 0.01\n",
      "multiply divide 0.1\n",
      "plead beg 0.07\n",
      "begin originate 0.48\n",
      "protect defend 0.26\n",
      "kill destroy 0.36\n",
      "create make 0.49\n",
      "accept reject 0.17\n",
      "ignore avoid 0.21\n",
      "carry bring 0.45\n",
      "leave enter 0.43\n",
      "choose elect 0.23\n",
      "lose fail 0.19\n",
      "encourage discourage 0.13\n",
      "achieve accomplish 0.19\n",
      "make construct 0.2\n",
      "listen obey 0.02\n",
      "inform notify 0.15\n",
      "receive give 0.49\n",
      "borrow beg 0.2\n",
      "take obtain 0.18\n",
      "advise recommend 0.17\n",
      "imitate portray 0.93\n",
      "win succeed 0.11\n",
      "think decide 0.32\n",
      "greet meet 0\n",
      "agree argue 0.23\n",
      "enjoy entertain 0.08\n",
      "destroy make 0.17\n",
      "save protect 0.42\n",
      "give lend 0.24\n",
      "understand know 0.41\n",
      "take receive 0.56\n",
      "accept acknowledge 0.15\n",
      "decide choose 0.24\n",
      "accept believe 0.37\n",
      "keep possess 0.16\n",
      "roam wander 0.25\n",
      "succeed fail 0.32\n",
      "spend save 0.42\n",
      "leave go 0.31\n",
      "come attend 0.19\n",
      "know believe 0.53\n",
      "gather meet 0.3\n",
      "make earn 0.31\n",
      "forget ignore 0.08\n",
      "multiply add 0.14\n",
      "shrink grow 0.18\n",
      "arrive leave 0.31\n",
      "succeed try 0.22\n",
      "accept deny 0.25\n",
      "arrive come 0.36\n",
      "agree differ 0.19\n",
      "send receive 0.39\n",
      "win dominate 0.27\n",
      "add divide 0.06\n",
      "kill choke 0.1\n",
      "acquire get 0.05\n",
      "participate join 0.35\n",
      "leave remain 0.46\n",
      "go enter 0.29\n",
      "take carry 0.44\n",
      "forget learn 0.22\n",
      "appoint elect 0.27\n",
      "engage marry 0.27\n",
      "ask pray 0.19\n",
      "go send 0.25\n",
      "take deliver 0.33\n",
      "speak hear 0.28\n",
      "analyze evaluate 0.19\n",
      "argue rationalize 0.56\n",
      "lose keep 0.38\n",
      "compare analyze 0.19\n",
      "disorganize organize 0.05\n",
      "go allow 0.4\n",
      "take possess 0.18\n",
      "learn listen 0.26\n",
      "destroy construct 0.23\n",
      "create build 0.41\n",
      "steal buy 0.18\n",
      "kill hang 0.14\n",
      "forget know 0.2\n",
      "create imagine 0.23\n",
      "do happen 0.35\n",
      "win accomplish 0.13\n",
      "give deny 0.26\n",
      "deserve earn 0.12\n",
      "get put 0.53\n",
      "locate find 0.14\n",
      "appear attend 0.3\n",
      "know comprehend 0.19\n",
      "pretend imagine 0.15\n",
      "satisfy please 0.19\n",
      "cherish keep 0\n",
      "argue differ 0.27\n",
      "overcome dominate 0.13\n",
      "behave obey 0.23\n",
      "cooperate participate 0.18\n",
      "achieve try 0.36\n",
      "fail discourage 0.07\n",
      "begin quit 0.24\n",
      "say participate 0.28\n",
      "come bring 0.53\n",
      "declare announce 0.15\n",
      "read comprehend 0.18\n",
      "take leave 0.5\n",
      "proclaim announce 0.97\n",
      "acquire obtain 0.26\n",
      "conclude decide 0.29\n",
      "please plead 0.13\n",
      "argue prove 0.36\n",
      "ask plead 0.23\n",
      "find disappear 0.21\n",
      "inspect examine 0.14\n",
      "verify justify 0.12\n",
      "assume predict 0.2\n",
      "learn evaluate 0.27\n",
      "argue justify 0.06\n",
      "make become 0.43\n",
      "discover originate 0.78\n",
      "achieve succeed 0.35\n",
      "give put 0.6\n",
      "understand listen 0.15\n",
      "expand grow 0.17\n",
      "borrow sell 0.2\n",
      "keep protect 0.45\n",
      "explain prove 0.28\n",
      "assume pretend 0.11\n",
      "agree please 0.29\n",
      "forgive forget 0.0\n",
      "clarify explain 0.14\n",
      "understand forgive 0.12\n",
      "remind forget 0.13\n",
      "get remain 0.48\n",
      "realize discover 0.24\n",
      "require inquire 0.04\n",
      "ignore ask 0.12\n",
      "think inquire 0.14\n",
      "reject avoid 0.32\n",
      "argue persuade 0.15\n",
      "pursue persuade 0.28\n",
      "accept forgive 0.33\n",
      "do quit 0.34\n",
      "investigate examine 0.24\n",
      "discuss explain 0.24\n",
      "owe lend 0.1\n",
      "explore discover 0.33\n",
      "complain argue 0.26\n",
      "withdraw reject 0.14\n",
      "keep borrow 0.2\n",
      "beg ask 0.08\n",
      "arrange organize 0.27\n",
      "reduce shrink 0.24\n",
      "speak acknowledge 0.24\n",
      "give borrow 0.34\n",
      "kill defend 0.27\n",
      "disappear shrink 0.18\n",
      "deliver carry 0.34\n",
      "breathe choke 0.29\n",
      "acknowledge notify 0.09\n",
      "become seem 0.4\n",
      "pretend seem 0.08\n",
      "accomplish become 0.05\n",
      "contemplate think 0.16\n",
      "determine predict 0.1\n",
      "please entertain 0.19\n",
      "remain retain 0.12\n",
      "pretend portray 0.31\n",
      "forget retain 0.17\n",
      "want choose 0.36\n",
      "lose get 0.5\n",
      "try think 0.52\n",
      "become appear 0.52\n",
      "leave ignore 0.24\n",
      "accept recommend 0.29\n",
      "leave wander 0.2\n",
      "keep give 0.54\n",
      "give allow 0.46\n",
      "bring send 0.31\n",
      "absorb learn 0.23\n",
      "acquire find 0.08\n",
      "leave appear 0.48\n",
      "create destroy 0.24\n",
      "begin go 0.38\n",
      "get buy 0.55\n",
      "collect save 0.28\n",
      "replace restore 0.14\n",
      "join add 0.36\n",
      "join marry 0.39\n",
      "accept deliver 0.33\n",
      "attach join 0.11\n",
      "put hang 0.23\n",
      "go sell 0.42\n",
      "communicate pray 0.11\n",
      "give steal 0.22\n",
      "add build 0.37\n",
      "bring restore 0.33\n",
      "comprehend satisfy 0.05\n",
      "portray decide 0.09\n",
      "organize become 0.13\n",
      "give know 0.55\n",
      "say verify 0.26\n",
      "cooperate join 0.29\n",
      "arrange require 0.16\n",
      "borrow want 0.1\n",
      "investigate pursue 0.32\n",
      "ignore explore 0.13\n",
      "bring complain 0.33\n",
      "enter owe 0.29\n",
      "portray notify 0.24\n",
      "remind sell 0.25\n",
      "absorb possess 0.12\n",
      "join acquire 0.33\n",
      "send attend 0.17\n",
      "gather attend 0.1\n",
      "absorb withdraw 0.16\n",
      "attend arrive 0.12\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for a,b in pairs:\n",
    "    score = sg.similarity(a,b)\n",
    "    y_pred.append(score)\n",
    "    print(a, b, round(score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--text', help='path containing training data', required=True)\n",
    "    parser.add_argument('--model', help='path to store/read model (when training/testing)', required=True)\n",
    "    parser.add_argument('--test', help='enters test mode', action='store_true')\n",
    "    opts = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    ## Please change the below file location to run the debias \n",
    "    f1 = open('C:\\\\NLP\\\\definitional_pairs.json')\n",
    "    definitional = json.load(f1)\n",
    "    \n",
    "    f2 = open('C:\\\\NLP\\\\gender_specific_seed.json')\n",
    "    gender_specific_words = json.load(f2)\n",
    "    \n",
    "    \n",
    "\n",
    "    if not opts.test:\n",
    "        sentences = text2sentences(opts.text)\n",
    "        sg = SkipGram(sentences)\n",
    "        sg.train()\n",
    "        \n",
    "        # Uncomment below line to run the debias before saving the embedding matrix\n",
    "        sg.debias(gender_specific_words, definitional)\n",
    "        \n",
    "        sg.save(opts.model)\n",
    "\n",
    "    else:\n",
    "        pairs = loadPairs(opts.text)\n",
    "\n",
    "        sg = SkipGram.load(opts.model)\n",
    "        \n",
    "        df = pd.read_csv(opts.text, sep ='\\t', engine='python')\n",
    "        \n",
    "        y_pred = []\n",
    "        y_test = df['similarity']\n",
    "\n",
    "        for a,b,_ in pairs:\n",
    "            # make sure this does not raise any exception, even if a or b are not in sg.vocab\n",
    "            score = sg.similarity(a,b)\n",
    "            y_pred.append(score)\n",
    "            print(score)\n",
    "            \n",
    "        print(np.corrcoef(y_pred,y_test)[0][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
